<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Big Data - An Introduction</title>
    <link rel="stylesheet" href="Style.css">
</head>
<body>
    <div>
        <span id= "title">
            <h1>Big Data - An Introduction</h1>
        </span>
        <span id= "topnav">
            <p>
                <a href="$sec1">Introduction</a>
                <a href="#sec2">Definition</a>
                <a href="#sec3">Characteristics</a>
            </p>
        </span>
    </div>
    <div id = "img">   
    <img src = "Big Data Image.jpg" alt = "Big Data in Context">
    </div>
    <div id= "sec1">
        <h2>Introduction</h2>
        <p>Big data primarily refers to data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many fields (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate.[2] Though used sometimes loosely partly because of a lack of formal definition, the interpretation that seems to best describe big data is the one associated with large body of information that we could not comprehend when used only in smaller amounts.[3]

            Big data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity.[4] The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, then the volume and variety of data can produce costs and risks that exceed an organization's capacity to create and capture value from big data.</p>
        </div>
    <div id = "sec2">
        <h2>Definition</h2>
        <p>The term <strong>big data</strong> has been in use <em>since the 1990s</em>, with some giving credit to <a href="https://en.wikipedia.org/wiki/John_Mashey" target ="_blank">John Mashey</a> for popularizing the term.[22][23] Big data usually includes data sets with sizes beyond the ability of commonly used software tools to capture, curate, manage, and process data within a tolerable elapsed time.[24] Big data philosophy encompasses unstructured, semi-structured and structured data; however, the main focus is on unstructured data.[25] Big data "size" is a constantly moving target; as of 2012 ranging from a few dozen terabytes to many zettabytes of data.[26] Big data requires a set of techniques and technologies with new forms of integration to reveal insights from data-sets that are diverse, complex, and of a massive scale.[27] <br> <br>
            "Variety", "veracity", and various other "Vs" are added by some organizations to describe it, a revision challenged by some industry authorities.[28] The Vs of big data were often referred to as the "three Vs", "four Vs", and "five Vs". They represented the qualities of big data in volume, variety, velocity, veracity, and value.[4] Variability is often included as an additional quality of big data. <br> <br>
            A 2018 definition states "Big data is where parallel computing tools are needed to handle data", and notes, "This represents a distinct and clearly defined change in the computer science used, via parallel programming theories, and losses of some of the guarantees and capabilities made by Codd's relational model."[29] <br> <br>
            In a comparative study of big datasets, Kitchin and McArdle found that none of the commonly considered characteristics of big data appear consistently across all of the analyzed cases.[30] For this reason, other studies identified the redefinition of power dynamics in knowledge discovery as the defining trait.[31] Instead of focusing on intrinsic characteristics of big data, this alternative perspective pushes forward a relational understanding of the object claiming that what matters is the way in which data is collected, stored, made available and analyzed.</p>
        </div>
    <div id = "sec3">
        <h2>Characteristics</h2>
        <hr /> <!--add dividing line-->
        <h2>Case Studies</h2>
        <div id= "table">
            <h3>Table of Contents</h3>
            <ul>
                <li>Volume</li>
                <li>Variety</li>
                <li>Velocity</li>
                <li>Veracity</li>
                <li>Value</li>
            </ul>
        </div>
        <p>Big data can be described by the following characteristics:</p>
        <h3>Volume</h3>
        <p>The quantity of generated and stored data. The size of the data determines the value and potential insight, and whether it can be considered big data or not. The size of big data is usually larger than terabytes and petabytes.</p>
        <h3>Variety</h3>
        <p>The type and nature of the data. The earlier technologies like RDBMSs were capable to handle structured data efficiently and effectively. However, the change in type and nature from structured to semi-structured or unstructured challenged the existing tools and technologies. The big data technologies evolved with the prime intention to capture, store, and process the semi-structured and unstructured (variety) data generated with high speed (velocity), and huge in size (volume). Later, these tools and technologies were explored and used for handling structured data also but preferable for storage. Eventually, the processing of structured data was still kept as optional, either using big data or traditional RDBMSs. This helps in analyzing data towards effective usage of the hidden insights exposed from the data collected via social media, log files, sensors, etc. Big data draws from text, images, audio, video; plus it completes missing pieces through data fusion.</p>
        <h3>Velocity</h3>
        <p>The speed at which the data is generated and processed to meet the demands and challenges that lie in the path of growth and development. Big data is often available in real-time. Compared to small data, big data is produced more continually. Two kinds of velocity related to big data are the frequency of generation and the frequency of handling, recording, and publishing.</p>
        <h3>Veracity</h3>
        <p>The truthfulness or reliability of the data, which refers to the data quality and the data value.[38] Big data must not only be large in size, but also must be reliable in order to achieve value in the analysis of it. The data quality of captured data can vary greatly, affecting an accurate analysis.</p>
    </div>
    
<a href="#">Back to the top</a>
</body>
</html>